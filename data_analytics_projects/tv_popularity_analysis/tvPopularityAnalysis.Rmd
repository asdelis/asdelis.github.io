---
title: "TV Show Popularity Analysis"
output: html_document
date: "2024-11-21"
---

ANDREW DELIS

Machine Learning Methods Implemented

1. Penalized Regression
2. Partition Clustering
3. Hierarchical Clustering
4. PCA + Partition Clustering
5. PCA + Hierarchical Clustering

What is the key contribution of each method of my analysis? 

1. Penalized regression was very valuable in helping me understand which variables were confounding and valuable to my target variable "popularity."

2. Partition clustering showed that popularity is heavily influenced by genre. When broadly looking at genre, sci-fi and childrens shows do well. The higher the number of groups drama (and the probably confounding 'soap' genre) become important to popularity.

3. Hierarchical clustering showed 3 natural groups. This was not apparent in the partition clustering.

4. PCA + Partition Clustering demonstrated how important the 'drama' (and the probably confounding 'reality') genre is to popularity. 

5. The PCA + Hierarchical Clustering visualizations illustrated that there are many smaller groups in the data. However, it is unclear if these influence popularity.

Did all methods support my conclusions?

There were not many instances of me being totally confused by a result. There were surprising results that prompted questions and further analysis, but overall genre was clearly very important when it came to popularity and the most popular genres kept popping up throughout my analysis.

### Set Up

#### Load Packages
```{r setup, include = FALSE}

# set up
knitr::opts_chunk$set( echo = TRUE )

# libraries
library( jsonlite )
library( dplyr )
library( tidyr )
library( caret )
library( rattle )
library( dataPreparation )
library( factoextra )
library( ggplot2 )
library( cluster )
library( fpc )
library( kernlab )
library( mclust )
library( gplots )
library( ggplot2 )
library( reshape2 )
library( RColorBrewer )
library( dendextend )
library( circlize )
library( NbClust )
library( lubridate )
library( caret )
library( mlbench )
library( doParallel )
library( fastDummies )
library( glmnet )
library( themis )
library( tidymodels )
library( cluster )
library( corrplot )
library( reshape2 )

```

#### Load Data
``` {r, include = FALSE}

# load data
con <- file( "tmdb_data.json", "r" )
data_raw <- stream_in( con, pagesize = 5 )
close( con )

```

#### Clean Data
``` {r }

# so I don't have to stream in the data every time I try to make cleaning changes
data <- data_raw

# un-nest the last_episode_to_air column and add prefix to avoid duplicate column names
data <- data |>
  unnest( last_episode_to_air, names_sep = "_" )
  
# convert episode_run_time to avg_episode_run_time
data$avg_episode_run_time <- sapply( data$episode_run_time, mean, na.rm = TRUE )

# keep only the first value in the origin_country lists
data$origin_country <- sapply( data$origin_country, `[`, 1 )

# create columns for number_of_genres, number_of_production_companies, and number_of_production_countries
data$number_of_genres <- sapply( data$genres, nrow )
data$number_of_production_companies <- sapply( data$production_companies, nrow )
data$number_of_production_countries <- sapply( data$production_countries, nrow )

# collapse genre, production_companies, and production_countries column
data$genres <- sapply( data$genres, 
                       function( x ) paste( x$name, collapse = ", " ) )

# one-hot encode genres
data <- dummy_cols( data, 
                    select_columns = "genres", 
                    split = ", ", 
                    remove_selected_columns = TRUE )

data$production_companies <- sapply( data$production_companies, 
                                     function( x ) paste( x$name, collapse = ", " ) )

data$production_countries <- sapply( data$production_countries, 
                                     function( x ) paste( x$name, collapse = ", " ) )

# change episode_run_time to average_episode_run_time
data$average_episode_run_time <- sapply( data$episode_run_time, function( x ) mean( unlist( x ) ) )

# change the type of the column
data$last_episode_to_air_air_date <- as.Date( data$last_episode_to_air_air_date )

```

#### Reduce Data
``` {r }

# remove irrelevant columns
data <- select( data, -c( id,
                          seasons,                    # already have "number_of_seasons"
                          tagline,
                          homepage,
                          networks,
                          overview, 
                          created_by,
                          poster_path,
                          spoken_languages,           # already have "original language"
                          backdrop_path,
                          languages,                  # already have "original language"
                          external_ids.imdb_id,
                          external_ids.tvdb_id,
                          external_ids.tvrage_id,
                          external_ids.twitter_id,
                          external_ids.facebook_id,
                          external_ids.freebase_id,
                          external_ids.wikidata_id,
                          external_ids.freebase_mid,
                          external_ids.instagram_id,
                          last_episode_to_air_still_path,
                          episode_run_time,
                          
                          # ends up not being super useful
                          # info is either redundant 
                          # or confounding when looking to make a new show
                          last_episode_to_air_production_code,
                          last_episode_to_air_id,
                          next_episode_to_air.production_code,
                          last_episode_to_air_name,
                          last_episode_to_air_overview,
                          last_episode_to_air_runtime,
                          last_episode_to_air_show_id,
                          last_episode_to_air_air_date,
                          last_episode_to_air_vote_count,
                          last_episode_to_air_vote_average,
                          last_episode_to_air_season_number,
                          last_episode_to_air_episode_number ) )

# filter data down to the last year
data <- data |>
  mutate( first_air_date = as.Date( first_air_date ) ) |>
  filter( first_air_date >= as.Date( "2023-11-12" ), 
          last_air_date <= as.Date( "2024-11-12" ) ) 

# remove next episode columns due to excessive null values
data <- select( data, -c( next_episode_to_air.id,
                          next_episode_to_air.name,
                          next_episode_to_air.runtime,
                          next_episode_to_air.show_id,
                          next_episode_to_air.air_date,
                          next_episode_to_air.overview,
                          next_episode_to_air.still_path,
                          next_episode_to_air.vote_count,
                          next_episode_to_air.episode_type,
                          next_episode_to_air.vote_average,
                          next_episode_to_air.season_number,
                          next_episode_to_air.episode_number ) )

# remove missing values
data <- na.omit( data )

```

#### Convert Some Columns to Factors
``` {r }

data$type <- as.factor( data$type )
data$status <- as.factor( data$status )
data$origin_country <- as.factor( data$origin_country )
data$original_language <- as.factor( data$original_language )

```

#### Calculate Target Variable
``` {r }

# find 90th percentile
cutoff <- quantile( data$popularity, 0.90 )

# find top 10% of popular tv shows
data$popular <- ifelse( data$popularity >= cutoff, 1, 0 )

# majority class
1 - mean( data$popular )

```

#### Data Overview
``` {r }

glimpse( data )

```

### Penalized Regression

#### Set Up
``` {r }

pr_data <- data |>
  select( -where( is.character ) ) |>
  select( -popular )

# set up the variables
x <- model.matrix( popularity ~ . - 1, data = pr_data )  
y <- pr_data$popularity

```

#### Inital Lasso Regression
``` {r }

# create lasso model w/ glmnet
lasso_model <- glmnet( x, y, alpha = 1 )

# cross-validate to find the optimal lambda
lasso_cv <- cv.glmnet( x, y, alpha = 1 )

# find the best lambda
best_lambda_lasso <- lasso_cv$lambda.min

# lasso model with best lambda
lasso_final <- glmnet( x, y, alpha = 1, lambda = best_lambda_lasso )

# results
print( "Lasso Regression Coefficients:" )
print( coef( lasso_final ) )
print( paste( "Best Lasso Lambda:", best_lambda_lasso ) )

```

#### Inital Ridge Regression
``` {r }

# create ridge model
ridge_model <- glmnet( x, y, alpha = 0 )

# cross-validate to find the optimal lambda
ridge_cv <- cv.glmnet( x, y, alpha = 0 )

# find the best lambda
best_lambda_ridge <- ridge_cv$lambda.min

# ridge model with best lambda
ridge_final <- glmnet( x, y, alpha = 0, lambda = best_lambda_ridge )

print( "Ridge Regression Coefficients:" )
print( coef( ridge_final ) )
print( paste( "Best Ridge Lambda:", best_lambda_ridge ) )

```

#### Remove Clearly Confounding Variables
``` {r }

pr_data <- data |>
  select( -where( is.character ) ) |>
  select( -c( popular, 
              number_of_episodes, 
              first_air_date, 
              vote_count, 
              vote_average,
              status,
              in_production ) )


# set up the variables
x <- model.matrix( popularity ~ . - 1, data = pr_data )  
y <- pr_data$popularity

```

#### Lasso Regression w/o Confounders
``` {r }

# create lasso model w/ glmnet
lasso_model <- glmnet( x, y, alpha = 1 )

# cross-validate to find the optimal lambda
lasso_cv <- cv.glmnet( x, y, alpha = 1 )

# find the best lambda
best_lambda_lasso <- lasso_cv$lambda.min

# lasso model with best lambda
lasso_final <- glmnet( x, y, alpha = 1, lambda = best_lambda_lasso )

# results
print( "Lasso Regression Coefficients:" )
print( coef( lasso_final ) )
print( paste( "Best Lasso Lambda:", best_lambda_lasso ) )

```

#### Ridge Regression w/o Confounders
``` {r }

# create ridge model
ridge_model <- glmnet( x, y, alpha = 0 )

# cross-validate to find the optimal lambda
ridge_cv <- cv.glmnet( x, y, alpha = 0 )

# find the best lambda
best_lambda_ridge <- ridge_cv$lambda.min

# ridge model with best lambda
ridge_final <- glmnet( x, y, alpha = 0, lambda = best_lambda_ridge )

print( "Ridge Regression Coefficients:" )
print( coef( ridge_final ) )
print( paste( "Best Ridge Lambda:", best_lambda_ridge ) )

```

#### Sorted Results
``` {r }

# extract coefficients from the models
ridge_coefs <- coef( ridge_final )
lasso_coefs <- coef( lasso_final )

# convert these to matrices and create dataframes
r_coef_df <- as.matrix( ridge_coefs )
r_coef_df <- data.frame( variable = rownames( r_coef_df ), 
                         coefficient = r_coef_df[,1], 
                         abs_coefficient = abs( r_coef_df[,1] ) )

l_coef_df <- as.matrix( lasso_coefs )
l_coef_df <- data.frame( variable = rownames( l_coef_df ), 
                       coefficient = l_coef_df[,1],
                       abs_coefficient = abs(l_coef_df[,1] ) )

# sort by absolute value in descending order
r_sorted_coefs <- r_coef_df[order( r_coef_df$abs_coefficient, decreasing = TRUE ),]
l_sorted_coefs <- l_coef_df[order( l_coef_df$abs_coefficient, decreasing = TRUE ),]


# print the sorted coefficients
print( r_sorted_coefs )
print( l_sorted_coefs )

```

#### Remove Outliers
``` {r }

# remove outliers and rerun
pr_data <- data |>
  select( -where( is.character ) ) |>
  select( -c( popular, 
              number_of_episodes, 
              first_air_date, 
              vote_count, 
              vote_average,
              status,
              in_production ) )

pr_data <- pr_data |>
  filter(between( popularity, 
                  quantile( popularity, 0.25 ) - 1.5 * IQR( popularity ),
                  quantile( popularity, 0.75 ) + 1.5 * IQR( popularity ) ) )

# set up the variables
x <- model.matrix( popularity ~ . - 1, data = pr_data )  
y <- pr_data$popularity

```

#### Lasso Regression w/o Confounders or Outliers
``` {r }

# create lasso model w/ glmnet
lasso_model <- glmnet( x, y, alpha = 1 )

# cross-validate to find the optimal lambda
lasso_cv <- cv.glmnet( x, y, alpha = 1 )

# find the best lambda
best_lambda_lasso <- lasso_cv$lambda.min

# lasso model with best lambda
lasso_final <- glmnet( x, y, alpha = 1, lambda = best_lambda_lasso )

# results
print( "Lasso Regression Coefficients:" )
print( coef( lasso_final ) )
print( paste( "Best Lasso Lambda:", best_lambda_lasso ) )

```

#### Ridge Regression w/o Confounders or Outliers
``` {r }

# create ridge model
ridge_model <- glmnet( x, y, alpha = 0 )

# cross-validate to find the optimal lambda
ridge_cv <- cv.glmnet( x, y, alpha = 0 )

# find the best lambda
best_lambda_ridge <- ridge_cv$lambda.min

# ridge model with best lambda
ridge_final <- glmnet( x, y, alpha = 0, lambda = best_lambda_ridge )

print( "Ridge Regression Coefficients:" )
print( coef( ridge_final ) )
print( paste( "Best Ridge Lambda:", best_lambda_ridge ) )

```

#### Sorted Results
``` {r }

# extract coefficients from the models
ridge_coefs <- coef( ridge_final )
lasso_coefs <- coef( lasso_final )

# convert these to matrices and create dataframes
r_coef_df <- as.matrix( ridge_coefs )
r_coef_df <- data.frame( variable = rownames( r_coef_df ), 
                         coefficient = r_coef_df[,1], 
                         abs_coefficient = abs( r_coef_df[,1] ) )

l_coef_df <- as.matrix( lasso_coefs )
l_coef_df <- data.frame( variable = rownames( l_coef_df ), 
                       coefficient = l_coef_df[,1],
                       abs_coefficient = abs(l_coef_df[,1] ) )

# sort by absolute value in descending order
r_sorted_coefs <- r_coef_df[order( r_coef_df$abs_coefficient, decreasing = TRUE ),]
l_sorted_coefs <- l_coef_df[order( l_coef_df$abs_coefficient, decreasing = TRUE ),]


# print the sorted coefficients
print( r_sorted_coefs )
print( l_sorted_coefs )

```

### Partition Clustering

#### Set Up w/ Siloutte and Elbow Methods
``` {r }

# filter data_num down based off previous analysis
pc_data <- data |>
  select( -where( is.character ) ) |>
  select( -c( popular, 
              number_of_episodes, 
              first_air_date, 
              vote_count, 
              vote_average,
              status,
              in_production ) )

# keep only the numeric columns
pc_data <- pc_data[, sapply( pc_data, is.numeric )]

# prepare your data (scaling if necessary)
pc_data <- scale( pc_data )

# check standard deviation of each column
# sd_cols <- apply( pc_data, 2, sd )
# sd_cols

# make sure hc_data is a data frame
pc_data <- as.data.frame( pc_data )

# remove columns that have no standard deviation
pc_data <- pc_data |>
  select( -c( genres_Romance,
              genres_History,
              genres_Musical ) )

# use elbow method to determine optimal number of clusters
set.seed( 123 )
fviz_nbclust( pc_data, kmeans, method = "wss" ) +
  geom_vline ( xintercept = 4, linetype = 2 ) +
  labs( subtitle = "Elbow Method" )

# silhouette method for additional cluster validation
fviz_nbclust( pc_data, kmeans, method = "silhouette" ) +
  labs( subtitle = "Silhouette Method" )

```

#### 2 Clusters
``` {r }

# perform k-means clustering
kmeans_result <- kmeans( pc_data, centers = 2, nstart = 25 )

# add cluster assignments to the original dataset
pc_data$cluster <- kmeans_result$cluster

# visualize the clustering results
pca_result <- prcomp( pc_data[, -ncol( pc_data )] )
fviz_cluster( kmeans_result, data = pc_data[, -ncol( pc_data )],
              geom = "point",
              ellipse.type = "convex",
              palette = "jco",
              ggtheme = theme_minimal() )

# summary of cluster sizes
table( pc_data$cluster )

# cluster centers
kmeans_result$centers

# calculate mean values for each cluster
cluster_means <- aggregate( . ~ cluster, data = pc_data, FUN = mean )
print( cluster_means )

# save group with highest popularity
high_pop_1 <- cluster_means |>
  filter( cluster == 1 )

```
#### 3 Clusters
``` {r }

# perform k-means clustering
kmeans_result <- kmeans( pc_data, centers = 3, nstart = 25 )

# add cluster assignments to the original dataset
pc_data$cluster <- kmeans_result$cluster

# visualize the clustering results
pca_result <- prcomp( pc_data[, -ncol( pc_data )] )
fviz_cluster( kmeans_result, data = pc_data[, -ncol( pc_data )],
              geom = "point",
              ellipse.type = "convex",
              palette = "jco",
              ggtheme = theme_minimal() )

# summary of cluster sizes
table( pc_data$cluster )

# cluster centers
kmeans_result$centers

# calculate mean values for each cluster
cluster_means <- aggregate( . ~ cluster, data = pc_data, FUN = mean )
print( cluster_means )

# save group with highest popularity
high_pop_2 <- cluster_means |>
  filter( cluster == 2 )

```

#### 4 Clusters
``` {r }

# perform k-means clustering
kmeans_result <- kmeans( pc_data, centers = 4, nstart = 25 )

# add cluster assignments to the original dataset
pc_data$cluster <- kmeans_result$cluster

# visualize the clustering results
pca_result <- prcomp( pc_data[, -ncol( pc_data )] )
fviz_cluster( kmeans_result, data = pc_data[, -ncol( pc_data )],
              geom = "point",
              ellipse.type = "convex",
              palette = "jco",
              ggtheme = theme_minimal() )

# summary of cluster sizes
table( pc_data$cluster )

# cluster centers
kmeans_result$centers

# calculate mean values for each cluster
cluster_means <- aggregate( . ~ cluster, data = pc_data, FUN = mean )
print( cluster_means )

# save group with highest popularity
high_pop_3 <- cluster_means |>
  filter( cluster == 3 )

```

#### 6 Clusters
``` {r }

# perform k-means clustering
kmeans_result <- kmeans( pc_data, centers = 6, nstart = 25 )

# add cluster assignments to the original dataset
pc_data$cluster <- kmeans_result$cluster

# visualize the clustering results
pca_result <- prcomp( pc_data[, -ncol( pc_data )] )
fviz_cluster( kmeans_result, data = pc_data[, -ncol( pc_data )],
              geom = "point",
              ellipse.type = "convex",
              palette = "jco",
              ggtheme = theme_minimal() )

# summary of cluster sizes
table( pc_data$cluster )

# cluster centers
kmeans_result$centers

# calculate mean values for each cluster
cluster_means <- aggregate( . ~ cluster, data = pc_data, FUN = mean )
print( cluster_means )

# save group with highest popularity
high_pop_4 <- cluster_means |>
  filter( cluster == 5 )

```

#### 10 Clusters
``` {r }

# perform k-means clustering
kmeans_result <- kmeans( pc_data, centers = 10, nstart = 25 )

# add cluster assignments to the original dataset
pc_data$cluster <- kmeans_result$cluster

# visualize the clustering results
pca_result <- prcomp( pc_data[, -ncol( pc_data )] )
fviz_cluster( kmeans_result, data = pc_data[, -ncol( pc_data )],
              geom = "point",
              ellipse.type = "convex",
              palette = "jco",
              ggtheme = theme_minimal() )

# summary of cluster sizes
table( pc_data$cluster )

# cluster centers
kmeans_result$centers

# calculate mean values for each cluster
cluster_means <- aggregate( . ~ cluster, data = pc_data, FUN = mean )
print( cluster_means )

# save group with highest popularity
high_pop_5 <- cluster_means |>
  filter( cluster == 8 )

```

``` {r }

pc_final_results <- rbind( high_pop_1, high_pop_2, high_pop_3, high_pop_4, high_pop_5 )
pc_final_results

```

### Hierarchical Clustering

#### Set Up
``` {r }

# filter data_num down based off previous analysis
hc_data <- data |>
  select( -where( is.character ) ) |>
  select( -c( popular, 
              number_of_episodes, 
              first_air_date, 
              vote_count, 
              vote_average,
              status,
              in_production ) )

# keep only the numeric columns
hc_data <- hc_data[, sapply( hc_data, is.numeric )]

# prepare your data (scaling if necessary)
hc_data <- scale( hc_data )

# check standard deviation of each column
# sd_cols <- apply( hc_data, 2, sd )
# sd_cols

# make sure hc_data is a data frame
hc_data <- as.data.frame( hc_data )

# remove columns that have no standard deviation
hc_data <- hc_data |>
  select( -c( genres_Romance,
              genres_History,
              genres_Musical ) )

# compute the dissimilarity matrix
d <- dist( hc_data, method = "euclidean" )

# hierarchical clustering using ward's method
hc <- hclust( d, method = "ward.D2" )

```

#### 2 Clusters
``` {r }

# plot the dendrogram
plot( hc, cex = 0.6, hang = -1 )

# cut the dendrogram into clusters
rect.hclust( hc, k = 2, border = 2:5 )
clusters <- cutree( hc, k = 2 )

# visualize the clusters
fviz_cluster( list( data = hc_data, cluster = clusters ) )

```

#### 3 Clusters
``` {r }

# plot the dendrogram
plot( hc, cex = 0.6, hang = -1 )

# cut the dendrogram into clusters
rect.hclust( hc, k = 3, border = 2:5 )
clusters <- cutree( hc, k = 3 )

# visualize the clusters
fviz_cluster( list( data = hc_data, cluster = clusters ) )

```

#### 4 Clusters
``` {r }

# plot the dendrogram
plot( hc, cex = 0.6, hang = -1 )

# cut the dendrogram into clusters
rect.hclust( hc, k = 4, border = 2:5 )
clusters <- cutree( hc, k = 4 )

# visualize the clusters
fviz_cluster( list( data = hc_data, cluster = clusters ) )

```

#### 6 Clusters
``` {r }

# plot the dendrogram
plot( hc, cex = 0.6, hang = -1 )

# cut the dendrogram into clusters
rect.hclust( hc, k = 6, border = 2:5 )
clusters <- cutree( hc, k = 6 )

# visualize the clusters
fviz_cluster( list( data = hc_data, cluster = clusters ) )

```

### PCA

#### PCA Calculation
``` {r }

# filter data_num down based off previous analysis
pca_data <- data |>
  select( -where( is.character ) ) |>
  select( -c( popular,
              popularity,
              number_of_episodes,
              number_of_seasons,
              first_air_date, 
              vote_count, 
              vote_average,
              status,
              in_production ) )

# keep only the numeric columns
data_num <- pca_data[, sapply( pca_data, is.numeric )]

# check standard deviation of each column
# apply(data_num, 2, sd)

# remove columns that have no standard deviation
data_num <- data_num |>
  select( -c( genres_Romance,
              genres_History,
              genres_Musical ) )

# standardize (mean zero and standard deviation 1) all the variables
scales <- build_scales( data_num, verbose = TRUE )
data_num <- fast_scale( data_num, scales = scales, verbose = TRUE )

# remove outlier function
remove_outliers <- function(x) {
    z_scores <- abs( scale( x ) )
    x[z_scores > 3] <- NA
    return( x )
}

# remove outliers
data_num <- as.data.frame( apply( data_num, 2, remove_outliers ) )

# remove na values
data_num <- na.omit( data_num )

# heat map variable
cor_data_num <- data_num

# calculate the covariance matrix for the numeric columns in the data set
cov <- cov( data_num )

# calculate Eigenvector and Eigenvalues of the variance-covariance matrix
e <- eigen( cov )
eigenvalues <- e$values
eigenvectors <- e$vectors
length( eigenvalues )

# estimate how much of variance is explained by each of the eigenvalues
var_explained <- round( eigenvalues / sum( eigenvalues ), 3 )

# calculate the cumulative variance explained by each component
cumulative_var <- cumsum( var_explained )
cumulative_var

# convert data_w to a matrix. Necessary for matrix multiplication
data_num <- as.matrix( data_num )

# heat map variable
full_pca <- data_num %*% eigenvectors[, c( 5:21 )]

# use only first 7 components (~90% of the data)
pca <- data_num %*% eigenvectors[,-c( 7:22 )]

# check whether there is any correlation in this new data
round( cor( pca ), 3 )

# convert to a data frame
pca_df <- as.data.frame( pca )

```

#### Heat Map
``` {r }

# create a correlation matrix variable
correlation_matrix <- cor( full_pca, cor_data_num )

# melt the correlation matrix for ggplot
melted_cormat <- melt( correlation_matrix )
colnames( melted_cormat ) <- c( "full_pca", "cor_data_num", "correlation" )

# Create the heatmap
ggplot(data = melted_cormat, aes( x = full_pca, 
                                  y = cor_data_num, 
                                  fill = correlation ) ) +
  geom_tile( color = "white" ) +
  scale_fill_gradient2( low = "red", 
                        mid = "white",
                        high = "blue",
                        midpoint = 0,
                        limit = c( -1, 1 ),
                        space = "Lab" ) +
  theme_minimal() +
  theme( axis.text.x = element_text( angle = 90, 
                                     vjust = 1, 
                                     size = 8, 
                                     hjust = 1 ), 
         axis.text.y = element_text( size = 8 ), 
         plot.title = element_text( hjust = 0.5 ) ) +
  labs(
    title = "Correlation Heatmap: full_pca vs cor_data_num",
    x = "full_pca Variables",
    y = "cor_data_num Variables",
    fill = "Correlation" ) +
  coord_fixed()

# save the plot
ggsave( "correlation_heatmap_different_axes.pdf", width = 20, height = 20 )

# Optional: Print out the most significant correlations
significant_correlations <- melted_cormat |>
  filter( abs( correlation ) > 0.5 ) |>
  arrange( desc( abs( correlation ) ) )

print( "Most Significant Correlations:" )
print( significant_correlations )

```

### PCA + Partition Clustering Set Up
``` {r }

# create a new variable
pca_pc_data <- pca_df

# use elbow method to determine optimal number of clusters
set.seed( 123 )
fviz_nbclust( pca_pc_data, kmeans, method = "wss" ) +
  geom_vline ( xintercept = 4, linetype = 2 ) +
  labs( subtitle = "Elbow Method" )

# silhouette method for additional cluster validation
fviz_nbclust( pca_pc_data, kmeans, method = "silhouette" ) +
  labs( subtitle = "Silhouette Method" )

```
#### 4 Clusters
``` {r }

# perform k-means clustering
kmeans_result <- kmeans( pca_pc_data, centers = 4, nstart = 25 )

# add cluster assignments to the original dataset
pca_pc_data$cluster <- kmeans_result$cluster

# visualize the clustering results
pca_result <- prcomp( pca_pc_data[, -ncol( pca_pc_data )] )
fviz_cluster( kmeans_result, data = pca_pc_data[, -ncol( pca_pc_data )],
              geom = "point",
              ellipse.type = "convex",
              palette = "jco",
              ggtheme = theme_minimal() )

# summary of cluster sizes
table( pca_pc_data$cluster )

# cluster centers
kmeans_result$centers

# calculate mean values for each cluster
cluster_means <- aggregate( . ~ cluster, data = pca_pc_data, FUN = mean )
print( cluster_means )

```

#### 10 Clusters
``` {r }

# perform k-means clustering
kmeans_result <- kmeans( pca_pc_data, centers = 10, nstart = 25 )

# add cluster assignments to the original dataset
pca_pc_data$cluster <- kmeans_result$cluster

# visualize the clustering results
pca_result <- prcomp( pca_pc_data[, -ncol( pca_pc_data )] )
fviz_cluster( kmeans_result, data = pca_pc_data[, -ncol( pca_pc_data )],
              geom = "point",
              ellipse.type = "convex",
              palette = "jco",
              ggtheme = theme_minimal() )

# summary of cluster sizes
table( pca_pc_data$cluster )

# cluster centers
kmeans_result$centers

# calculate mean values for each cluster
cluster_means <- aggregate( . ~ cluster, data = pca_pc_data, FUN = mean )
print( cluster_means )

```

### PCA + Hierarchical Clustering

#### 4 Clusters
``` {r }

# create a new variable
pca_hc_data <- pca_df

# compute the dissimilarity matrix
d <- dist( pca_hc_data, method = "euclidean" )

# hierarchical clustering using ward's method
hc <- hclust( d, method = "ward.D2" )

# plot the dendrogram
plot( hc, cex = 0.6, hang = -1 )

# cut the dendrogram into clusters
rect.hclust( hc, k = 4, border = 2:5 )
clusters <- cutree( hc, k = 4 )

# visualize the clusters
fviz_cluster( list( data = pca_hc_data, cluster = clusters ) )

```

#### 10 Clusters
``` {r }

# create a new variable
pca_hc_data <- pca_df

# compute the dissimilarity matrix
d <- dist( pca_hc_data, method = "euclidean" )

# hierarchical clustering using ward's method
hc <- hclust( d, method = "ward.D2" )

# plot the dendrogram
plot( hc, cex = 0.6, hang = -1 )

# cut the dendrogram into clusters
rect.hclust( hc, k = 10, border = 2:5 )
clusters <- cutree( hc, k = 10 )

# visualize the clusters
fviz_cluster( list( data = pca_hc_data, cluster = clusters ) )

```
